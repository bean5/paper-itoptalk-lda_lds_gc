% This is annote.bib
% Author: Michael Bean
% The order of the following entries is irrelevant. They will be sorted according to the
% bibliography style used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%@string{jgr = "J.~Geophys.~Res."}

@article{hilton-2008-intertext-abinadi,
	AUTHOR = {Hilton III, John},
	YEAR = {2008},
	TITLE = {Textual Similarities in the Words of Abinadi and Alma’s Counsel to Corianton},
	URL = {http://www.johnhiltoniii.com/wp-content/uploads/2008/10/51.2-HiltonIII-Textual-Similarities.pdf}
}

@article{hilton_2008_intertext_psalms,
	AUTHOR = {Hilton III, John},
	YEAR = {2008},
	TITLE = {Old Testament Psalms in the Book of Mormon},
	URL = {http://www.johnhiltoniii.com/wp-content/uploads/2013/10/Hilton-Old-Testament-Psalms-in-the-Book-of-Mormon-Final.pdf}
}

%xyz post online
@online{bean5-LDA-ToT,
	AUTHOR = {Bean, Michael and Ringger, Eric},
	TITLE = {Theological topics through time: An application of Gibbs-sampled LDA and post-hoc metrics to compare religious venues},
	Month = Dec,
	Year = {2013}
	%NOTE = "[online; Accessed on 2014-09-01]",
	%URL = {http://bean5.github.io/machine-learning/food-classification/index.html}
}

%xyz post online
@online{bean5-LDA-kNN,
	AUTHOR = {Michael Bean},
	TITLE = {Ranking Potential Recommendations in a Religious Textual Context in the Probability Simplex: LDA, Gibbs Sampling, k-NN, and Hellinger Distance},
	Month = Apr,
	Year = {2014}
	%NOTE = "[online]",
	%URL = {http://bean5.github.io/machine-learning/food-classification/index.html}
}

@article{davies-trend-detected,
    Abstract = {Within the past decade several large, freely-available online corpora of Spanish and Portuguese have become available. With these new corpora, researchers of Spanish and Portuguese can now carry out the same type of corpus-based research that has been done for other languages (such as English) for years. This includes advanced research on morphological and syntactic variation (thanks to full functionality with substring searches, part of speech tagging, and lemmatization), semantics and pragmatics (via collocates, synonyms, customized word lists, and word comparisons), and historical changes and synchronic register variation (via architectures and interfaces that allow easy comparisons of frequency in different sections of the corpus). [ABSTRACT FROM AUTHOR]},
    Author = {Davies, Mark},
    ISSN = {19390238},
    Journal = {Studies in Hispanic \& Lusophone Linguistics},
    Keywords = {LINGUISTICS, SYNTAX (Grammar), SEMANTICS, PRAGMATICS, SPANISH language, PORTUGUESE language},
    Number = {1},
    Pages = {149 - 186},
    Title = {New Directions in Spanish and Portuguese Corpus Linguistics.},
    Volume = {1},
    URL = {https://www.lib.byu.edu/cgi-bin/remoteauth.pl?url=http://search.ebscohost.com/login.aspx?direct=true&db=ufh&AN=31408139&site=ehost-live&scope=site},
    Year = {2008},
}

@online{about-jod,
	AUTHOR = {The Church of Jesus Christ of Latter-day Saints},
	TITLE = {Journal of Discourses},
%	Month = Sept,
	Year = {2014},
	NOTE = "[online; Accessed on 2014-09-01]",
	URL = {https://www.lds.org/topics/journal-of-discourses?lang=eng}
}

@online{sci,
	AUTHOR = {Liddle, Stephen},
	TITLE = {LDS Scripture Citation Index},
	Month = oct,
	Year = {2013},
	NOTE = "[online; Accessed on 2013-10-24]",
	URL = {http://scriptures.byu.edu/}
}

@misc{lucene:luke,
	AUTHOR = {},
	TITLE = {Luke - Lucene Index Toolbox},
	Month = nov,
	Year = "2013",
	NOTE = "[online; Accessed on 2013-11-14]",
	URL = "http://code.google.com/p/luke/"
}

@book{McCandless:2010:LAS:1893016,
	author = {McCandless, Michael and Hatcher, Erik and Gospodnetic, Otis},
	title = {Lucene in Action, Second Edition: Covers Apache Lucene 3.0},
	year = {2010},
	isbn = {1933988177, 9781933988177},
	publisher = {Manning Publications Co.},
	address = {Greenwich, CT, USA},
}

@inproceedings{Porteous:2008:FCG:1401890.1401960,
    author = {Porteous, Ian and Newman, David and Ihler, Alexander and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
    title = {Fast Collapsed Gibbs Sampling for Latent Dirichlet Allocation},
    booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    series = {KDD '08},
    year = {2008},
    isbn = {978-1-60558-193-4},
    location = {Las Vegas, Nevada, USA},
    pages = {569--577},
    numpages = {9},
    url = {http://doi.acm.org/10.1145/1401890.1401960},
    doi = {10.1145/1401890.1401960},
    acmid = {1401960},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {latent dirichlet allocation, sampling},
}

@article{Blei:2003:LDA:944919.944937,
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	title = {Latent Dirichlet Allocation},
	journal = {J. Mach. Learn. Res.},
	issue_date = {3/1/2003},
	volume = {3},
	month = mar,
	year = {2003},
	issn = {1532-4435},
	pages = {993--1022},
	numpages = {30},
	url = {http://dl.acm.org/citation.cfm?id = 944919.944937},
	acmid = {944937},
	publisher = {JMLR.org},
}

@book{Blei2007Handbook,
	editor = {}, 
	author = {David M. Blei},
	booktitle = {Introduction to Probabilistic Topic Models},
	title = {Introduction to Probabilistic Topic Models},
	publisher = {},
	address = {},
	year = 2007,
	url = {http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf},
	abstract = {Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents. In this article, we review the main ideas of this field, survey the current state-of-the-art, and describe some promisingfuture directions. We first describe latent Dirichlet allocation (LDA) [8], which is the simplest kind of topic model. We discuss its connections to probabilistic modeling,and describe two kinds of algorithms for topic discovery. We then survey the growing body of research that extends and applies topic models in interesting ways. These extensions have been developed by relaxing some of the statistical assumptions of LDA, incorporating meta-data into the analysis of the documents, and using similar kinds of models on a diversity of data types such as social networks, images and genetics. Finally, we give our thoughts as to some of the important unexplored directions for topic modeling. These include rigorous methods for checking models built for data exploration, new approaches to visualizing text and other high dimensional data, and moving beyond traditional information engineering applications towards using topic models for more scientific ends.},
	annote = {This handbook is 16 pages long and covers an introduction to the suite of algorithms known as Probabilistic topic models whose ``aim is to discover the hidden thematic structure in large archives of documents.'' This survey includes the current state-of-the-art, and describe some promising future directions....LDA...and describe two kinds of algorithms for topic discovery. We then survey the growing body of research that extends and applies topic models in interesting ways. These extensions have been developed by relaxing some of the statistical assumptions of LDA, incorporating meta-data into the analysis of the documents, and using similar kinds of models on a diversity of data types such as social networks, images and genetics.'' Future directions are also described including ``rigorous methods for checking models built for data exploration, new approaches to visualizing text..., and moving beyond traditional information engineering applications towards using topic models for more scientific ends.''}
}

@misc{Blei2006Dynamic,
	author = {David M. Blei and John D. Lafferty},
	title = {Dynamic Topic Models},
	year = {2006},
	url = {http://dl.acm.org/ft_gateway.cfm?id = 1143859&ftid = 364240&dwn = 1&CFID = 251978667&CFTOKEN = 17214624},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR’ed archives of the journal Science from 1880 through 2000.},
	annote = {``A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections....In addition to giving quantitative, predictive models of sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection.'' The models are by analyzing 120 120 years of journals.}
}

@inproceedings{Newman10automaticevaluation,
	author = {David Newman and Jey Han Lau and Karl Grieser and Timothy Baldwin},
	title = {Automatic evaluation of topic coherence},
	booktitle = {In NAACL-HLT},
	year = {2010},
	url = {http://aclweb.org/anthology/N/N10/N10-1012.pdf},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point-wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	annote = {``This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability....In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point-wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation.}
}

@inproceedings{hall-jurafsky-manning:2008:EMNLP,
	author = {Hall, David and Jurafsky, Daniel and Manning, Christopher D.},
	title = {Studying the History of Ideas Using Topic Models},
	booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
	month = {October},
	year = {2008},
	address = {Honolulu, Hawaii},
	publisher = {Association for Computational Linguistics},
	pages = {363--371},
	url = {http://www.aclweb.org/anthology/D08-1038},
	abstract = {How can the development of ideas in a scientific field be studied over time? We apply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time. Our methods find trends in the field including the rise of probabilistic methods starting in 1988, a steady increase in applications, and a sharp decline of research in semantics and understanding between 1978 and 2001, possibly rising again after 2001. We also introduce a model of the diversity of ideas, topic entropy, using it to show that COLING is a more diverse conference than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon divergence of topic distributions to show that all three conferences are converging in the topics they cover. },
	annote = {``We apply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time...We also introduce a model of the diversity of ideas, \textit{topic entropy}'' then use it empirically to show that some topics in a field are more or less diverse.	When then ``apply Jensen-Shannon divergence of topic distributions to show that ... conferences are converging in the topics they cover.''}
}

@inproceedings{asgari-chappelier:2013:CLfL,
	author = {Asgari, Ehsaneddin and Chappelier, Jean-Cedric},
	title = {Linguistic Resources \& Topic Models for the Analysis of Persian Poems},
	booktitle = {Proceedings of the Workshop on Computational Linguistics for Literature},
	month = {June},
	year = {2013},
	address = {Atlanta, Georgia},
	publisher = {Association for Computational Linguistics},
	pages = {23--31},
	url = {http://www.aclweb.org/anthology/W13-1404},
	abstract = {This paper describes the usage of Natural Language Processing tools, mostly probabilistic topic modeling, to study semantics (word correlations) in a collection of Persian poems consisting of roughly 18k poems from 30 different poets. For this study, we put a lot of effort in the preprocessing and the development of a large scope lexicon supporting both modern and ancient Persian. In the analysis step,we obtained very interesting and meaningful results regarding the correlation between poets and topics, their evolution through time,as well as the correlation between the topics and the metre used in the poems. This work should thus provide valuable results to literature researchers, especially for those working on stylistics or comparative literature.},
	annote = {``This paper describes the usage of Natural Language Processing tools...to study semantics (word correlations) in a collection of Persian poems...both modern and ancient...In the analysis step, we obtained very interesting and meaningful results regarding the correlation between poets and topics, their \textit{evolution through time}, as well as the correlation between the topics and the metre used in the poems.'' }
}

@unpublished{McCallumMALLET,
    author = "Andrew Kachites McCallum",
    title = "MALLET: A Machine Learning for Language Toolkit",
    note = "http://mallet.cs.umass.edu",
    year = 2002
}

%Very good paper. We use their metrics.
%http://dl.acm.org/citation.cfm?id = 1864761
@inproceedings{Ge:2010:BAE:1864708.1864761,
	author = {Ge, Mouzhi and Delgado-Battenfeld, Carla and Jannach, Dietmar},
	title = {Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity},
	booktitle = {Proceedings of the Fourth ACM Conference on Recommender Systems},
	series = {RecSys '10},
	year = {2010},
	isbn = {978-1-60558-906-0},
	location = {Barcelona, Spain},
	pages = {257--260},
	numpages = {4},
	url = {http://doi.acm.org/10.1145/1864708.1864761},
	doi = {10.1145/1864708.1864761},
	acmid = {1864761},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {coverage, evaluation metric, recommender system, serendipity}
}

%http://dl.acm.org/citation.cfm?doid = 1864708.1864721
@inproceedings{Cremonesi:2010:PRA:1864708.1864721,
	author = {Cremonesi, Paolo and Koren, Yehuda and Turrin, Roberto},
	title = {Performance of Recommender Algorithms on Top-n Recommendation Tasks},
	booktitle = {Proceedings of the Fourth ACM Conference on Recommender Systems},
	series = {RecSys '10},
	year = {2010},
	isbn = {978-1-60558-906-0},
	location = {Barcelona, Spain},
	pages = {39--46},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1864708.1864721},
	doi = {10.1145/1864708.1864721},
	acmid = {1864721},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {evaluation, precision, recall, top-n recommendations}
}

%Can be used to improve search, because it can tag a document with keywords. But since they are binary (present or not), it is ends up being a subset of topic modelling, where the indicator function is a threshold: For each tag, If the document contains more than x% of topic y, then tag it with y. //Otherwise, do not tag it. While this might be sufficient for search, it doesn’t appear to be sufficient for a recommendation.
%http://dl.acm.org/citation.cfm?id = 1639726
@inproceedings{Krestel:2009:LDA:1639714.1639726,
	author = {Krestel, Ralf and Fankhauser, Peter and Nejdl, Wolfgang},
	title = {Latent Dirichlet Allocation for Tag Recommendation},
	booktitle = {Proceedings of the Third ACM Conference on Recommender Systems},
	series = {RecSys '09},
	year = {2009},
	isbn = {978-1-60558-435-5},
	location = {New York, New York, USA},
	pages = {61--68},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1639714.1639726},
	doi = {10.1145/1639714.1639726},
	acmid = {1639726},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {delicious, social bookmarking system, tag recommendation, tag search}
}

%Word-sense disambiguation, WSD
@inproceedings{Brown:1991:WDU:981344.981378,
    author = {Brown, Peter F. and Pietra, Stephen A. Della and Pietra, Vincent J. Della and Mercer, Robert L.},
    title = {Word-sense Disambiguation Using Statistical Methods},
    booktitle = {Proceedings of the 29th Annual Meeting on Association for Computational Linguistics},
    series = {ACL '91},
    year = {1991},
    location = {Berkeley, California},
    pages = {264--270},
    numpages = {7},
    url = {http://dx.doi.org/10.3115/981344.981378},
    doi = {10.3115/981344.981378},
    acmid = {981378},
    publisher = {Association for Computational Linguistics},
    address = {Stroudsburg, PA, USA},
}

@inproceedings{Sanderson:1994:WSD:188490.188548,
    author = {Sanderson, Mark},
    title = {Word Sense Disambiguation and Information Retrieval},
    booktitle = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
    series = {SIGIR '94},
    year = {1994},
    isbn = {0-387-19889-X},
    location = {Dublin, Ireland},
    pages = {142--151},
    numpages = {10},
    url = {http://dl.acm.org/citation.cfm?id=188490.188548},
    acmid = {188548},
    publisher = {Springer-Verlag New York, Inc.},
address = {New York, NY, USA},
}